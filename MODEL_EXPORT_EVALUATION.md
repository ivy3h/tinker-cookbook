# æ¨¡å‹å¯¼å‡ºå’Œè¯„ä¼°æŒ‡å—

## ğŸ“¦ æ¨¡å‹å¯¼å‡º

Tinker æ”¯æŒå¯¼å‡ºè®­ç»ƒå¥½çš„ LoRA æƒé‡ï¼Œä½ å¯ä»¥åœ¨å…¶ä»–æ¨ç†å¹³å°ä½¿ç”¨è¿™äº›æƒé‡ã€‚

### æ–¹æ³• 1: ä½¿ç”¨ model_path å¯¼å‡ºï¼ˆæ¨èï¼‰

è®­ç»ƒå®Œæˆåï¼Œ`sampling_client` ä¼šæœ‰ä¸€ä¸ª `model_path` å±æ€§ï¼š

```python
# è®­ç»ƒå®Œæˆå
sampling_client = training_client.save_weights_and_get_sampling_client(
    name='my-finetuned-model'
)

# æ‰“å°æ¨¡å‹è·¯å¾„ï¼ˆç±»ä¼¼ tinker://<unique_id>/sampler_weights/finalï¼‰
print(f"Model path: {sampling_client.model_path}")

# ä¸‹è½½æ¨¡å‹æƒé‡
import tinker

service_client = tinker.ServiceClient()
rest_client = service_client.create_rest_client()

# ä¸‹è½½æƒé‡
future = rest_client.download_checkpoint_archive_from_tinker_path(
    sampling_client.model_path
)
archive_data = future.result()

# ä¿å­˜ä¸º tar.gz æ–‡ä»¶
with open("model-checkpoint.tar.gz", "wb") as f:
    f.write(archive_data)

print("Model downloaded successfully!")
```

### æ–¹æ³• 2: ä½¿ç”¨ checkpoint ID å¯¼å‡º

å¦‚æœä½ çŸ¥é“ checkpoint IDï¼š

```python
import tinker

service_client = tinker.ServiceClient()
rest_client = service_client.create_rest_client()

# æ›¿æ¢ <unique_id> ä¸ºä½ çš„ checkpoint ID
checkpoint_path = "tinker://<unique_id>/sampler_weights/final"

future = rest_client.download_checkpoint_archive_from_tinker_path(
    checkpoint_path
)
archive_data = future.result()

with open("model-checkpoint.tar.gz", "wb") as f:
    f.write(archive_data)
```

### å¯¼å‡ºçš„å†…å®¹

ä¸‹è½½çš„ `model-checkpoint.tar.gz` æ–‡ä»¶åŒ…å«ï¼š
- LoRA adapter æƒé‡
- é…ç½®æ–‡ä»¶
- å…¶ä»–è®­ç»ƒä¿¡æ¯

### è§£å‹å’Œä½¿ç”¨

```bash
# è§£å‹æ–‡ä»¶
mkdir model_weights
tar -xzvf model-checkpoint.tar.gz -C model_weights

# æŸ¥çœ‹å†…å®¹
ls model_weights/
```

### ä¸å…¶ä»–æ¡†æ¶é›†æˆ

**ä½¿ç”¨ HuggingFace transformers:**

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-30B-A3B-Base")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-30B-A3B-Base")

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, "./model_weights")

# åˆå¹¶æƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºéƒ¨ç½²ï¼‰
merged_model = model.merge_and_unload()

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
save_path = "./merged_model"
merged_model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
```

**ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†:**

```python
from vllm import LLM, SamplingParams

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹
llm = LLM(model="./merged_model")

# æ¨ç†
sampling_params = SamplingParams(temperature=0.7, max_tokens=512)
outputs = llm.generate(["Your prompt here"], sampling_params)
```

---

## ğŸ“Š æ¨¡å‹è¯„ä¼°

Tinker æä¾›äº†è¯„ä¼°å·¥å…·ï¼Œç‰¹åˆ«æ˜¯ä¸ InspectAI çš„é›†æˆã€‚

### 1. åŸºç¡€è¯„ä¼°æ–¹æ³•

#### åœ¨è®­ç»ƒå¾ªç¯ä¸­è¯„ä¼°

```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¯„ä¼°
for epoch in range(num_epochs):
    # è®­ç»ƒæ­¥éª¤
    fwdbwd_future = training_client.forward_backward(
        processed_examples, 
        "cross_entropy"
    )
    optim_future = training_client.optim_step(
        types.AdamParams(learning_rate=learning_rate)
    )
    
    # è®¡ç®—è®­ç»ƒæŸå¤±
    fwdbwd_result = fwdbwd_future.result()
    # ... è®¡ç®—æŸå¤± ...
    
    # æ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    if epoch % 5 == 0:
        # ä¿å­˜æ£€æŸ¥ç‚¹å¹¶è¯„ä¼°
        eval_sampling_client = training_client.save_weights_and_get_sampling_client(
            name=f'checkpoint-epoch-{epoch}'
        )
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        eval_results = evaluate_model(eval_sampling_client, validation_data)
        print(f"Epoch {epoch} - Validation accuracy: {eval_results['accuracy']}")
```

#### è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°

```python
def evaluate_model(sampling_client, test_data, tokenizer):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        sampling_client: Tinker sampling client
        test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        tokenizer: tokenizer å¯¹è±¡
    
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    correct = 0
    total = 0
    
    for example in test_data:
        # æ„å»º prompt
        instruction = example.get("instruction", "")
        input_text = example.get("input", "")
        expected_output = example.get("output", "")
        
        if input_text:
            prompt_text = f"Instruction: {instruction}\nInput: {input_text}\nResponse:"
        else:
            prompt_text = f"Instruction: {instruction}\nResponse:"
        
        # ç”Ÿæˆé¢„æµ‹
        prompt_tokens = tokenizer.encode(prompt_text)
        prompt = types.ModelInput.from_ints(prompt_tokens)
        
        params = types.SamplingParams(
            max_tokens=512,
            temperature=0.0,  # ä½¿ç”¨è´ªå©ªè§£ç è·å¾—ç¡®å®šæ€§è¾“å‡º
            stop=["\n\n", "Instruction:"]
        )
        
        future = sampling_client.sample(
            prompt=prompt,
            sampling_params=params,
            num_samples=1
        )
        result = future.result()
        
        # è·å–æ¨¡å‹è¾“å‡º
        model_output = tokenizer.decode(result.sequences[0].tokens).strip()
        
        # è¯„ä¼°ï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªå®šä¹‰ï¼‰
        if check_answer(model_output, expected_output):
            correct += 1
        total += 1
    
    accuracy = correct / total if total > 0 else 0
    return {
        "accuracy": accuracy,
        "correct": correct,
        "total": total
    }

def check_answer(model_output, expected_output):
    """
    æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
    æ ¹æ®ä½ çš„ä»»åŠ¡ç±»å‹è‡ªå®šä¹‰æ­¤å‡½æ•°
    """
    # å¯¹äºæ•°å­¦é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æå–æœ€ç»ˆç­”æ¡ˆ
    # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ç±»åˆ«æ ‡ç­¾
    # ç®€å•ç¤ºä¾‹ï¼šå­—ç¬¦ä¸²åŒ¹é…
    return model_output.strip().lower() == expected_output.strip().lower()
```

### 2. ä½¿ç”¨ Tinker Cookbook çš„è¯„ä¼°å·¥å…·

Tinker Cookbook æä¾›äº†æ›´é«˜çº§çš„è¯„ä¼°æŠ½è±¡ï¼š

```python
from tinker_cookbook.evaluation import evaluate_completions

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = evaluate_completions(
    sampling_client=sampling_client,
    test_dataset=test_data,
    renderer=renderer,  # æ¥è‡ª tinker_cookbook.renderers
    metrics=['accuracy', 'exact_match', 'f1']
)

# è¿è¡Œè¯„ä¼°
results = evaluator.run()

print(f"Accuracy: {results['accuracy']:.4f}")
print(f"Exact Match: {results['exact_match']:.4f}")
print(f"F1 Score: {results['f1']:.4f}")
```

### 3. ä¸ InspectAI é›†æˆï¼ˆæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼‰

Tinker Cookbook æ”¯æŒä¸ InspectAI é›†æˆï¼Œç”¨äºåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼š

```python
from tinker_cookbook.inspect_evaluation import run_inspect_eval

# åœ¨ MMLUã€GSM8K ç­‰æ ‡å‡†åŸºå‡†ä¸Šè¯„ä¼°
results = run_inspect_eval(
    sampling_client=sampling_client,
    benchmark="gsm8k",  # æˆ– "mmlu", "hellaswag" ç­‰
    batch_size=32
)

print(f"GSM8K Score: {results['score']:.2f}%")
```

### 4. æ•°å­¦æ¨ç†è¯„ä¼°ï¼ˆé’ˆå¯¹ä½ çš„æ•°æ®ï¼‰

å¯¹äºæ•°å­¦è¯æ˜å’Œæ¨ç†ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨ç‰¹å®šçš„è¯„ä¼°æ–¹æ³•ï¼š

```python
def evaluate_math_reasoning(sampling_client, test_data, tokenizer):
    """
    è¯„ä¼°æ•°å­¦æ¨ç†ä»»åŠ¡
    
    è¯„ä¼°æŒ‡æ ‡ï¼š
    - æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ç‡
    - æ¨ç†æ­¥éª¤è´¨é‡
    - å®Œæ•´æ€§
    """
    results = {
        'correct_answer': 0,
        'correct_steps': 0,
        'complete_proof': 0,
        'total': 0
    }
    
    for example in test_data:
        instruction = example.get("instruction", "")
        expected_output = example.get("output", "")
        
        # ç”Ÿæˆæ¨¡å‹è¾“å‡º
        prompt_text = f"Instruction: {instruction}\nResponse:"
        prompt_tokens = tokenizer.encode(prompt_text)
        prompt = types.ModelInput.from_ints(prompt_tokens)
        
        params = types.SamplingParams(
            max_tokens=2048,  # æ•°å­¦æ¨ç†éœ€è¦è¾ƒé•¿è¾“å‡º
            temperature=0.0
        )
        
        future = sampling_client.sample(
            prompt=prompt,
            sampling_params=params,
            num_samples=1
        )
        result = future.result()
        model_output = tokenizer.decode(result.sequences[0].tokens)
        
        # è¯„ä¼°ä¸åŒæ–¹é¢
        if check_final_answer(model_output, expected_output):
            results['correct_answer'] += 1
        
        if check_reasoning_steps(model_output, expected_output):
            results['correct_steps'] += 1
        
        if check_completeness(model_output):
            results['complete_proof'] += 1
        
        results['total'] += 1
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    total = results['total']
    return {
        'answer_accuracy': results['correct_answer'] / total,
        'step_accuracy': results['correct_steps'] / total,
        'completeness': results['complete_proof'] / total
    }

def check_final_answer(model_output, expected_output):
    """æå–å¹¶æ¯”è¾ƒæœ€ç»ˆç­”æ¡ˆ"""
    # å®ç°ç­”æ¡ˆæå–é€»è¾‘
    pass

def check_reasoning_steps(model_output, expected_output):
    """æ£€æŸ¥æ¨ç†æ­¥éª¤æ˜¯å¦æ­£ç¡®"""
    # å®ç°æ­¥éª¤éªŒè¯é€»è¾‘
    pass

def check_completeness(model_output):
    """æ£€æŸ¥è¯æ˜æ˜¯å¦å®Œæ•´"""
    # å®ç°å®Œæ•´æ€§æ£€æŸ¥é€»è¾‘
    pass
```

### 5. è¯„ä¼°æœ€ä½³å®è·µ

#### åˆ’åˆ†æ•°æ®é›†

```python
# å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split

all_data = load_json_files(["data/s1k.json"])

train_data, test_data = train_test_split(
    all_data,
    test_size=0.2,  # 20% ä½œä¸ºæµ‹è¯•é›†
    random_state=42
)

print(f"Training examples: {len(train_data)}")
print(f"Test examples: {len(test_data)}")
```

#### ä¿å­˜æµ‹è¯•é›†

```python
import json

# ä¿å­˜æµ‹è¯•é›†ä»¥ä¾¿åç»­ä½¿ç”¨
with open("data/test_set.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False, indent=2)
```

#### å®šæœŸè¯„ä¼°

```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
evaluation_history = []

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    
    # æ¯ 5 ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    if epoch % 5 == 0:
        eval_results = evaluate_model(sampling_client, test_data, tokenizer)
        evaluation_history.append({
            'epoch': epoch,
            'accuracy': eval_results['accuracy'],
            'loss': current_loss
        })
        
        print(f"Epoch {epoch} - Test Accuracy: {eval_results['accuracy']:.4f}")

# ä¿å­˜è¯„ä¼°å†å²
with open("evaluation_history.json", "w") as f:
    json.dump(evaluation_history, f, indent=2)
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹ï¼šè®­ç»ƒã€å¯¼å‡ºã€è¯„ä¼°

```python
import tinker
from tinker import types
import json

# 1. è®­ç»ƒæ¨¡å‹
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
    base_model="Qwen/Qwen3-30B-A3B-Instruct",
    rank=32
)

# ... è®­ç»ƒè¿‡ç¨‹ ...

# 2. ä¿å­˜å¹¶è·å– sampling client
sampling_client = training_client.save_weights_and_get_sampling_client(
    name='math-model-final'
)
model_path = sampling_client.model_path
print(f"Model saved at: {model_path}")

# 3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
test_data = load_json_files(["data/test_set.json"])
eval_results = evaluate_model(sampling_client, test_data, tokenizer)
print(f"Test Accuracy: {eval_results['accuracy']:.4f}")

# 4. å¯¼å‡ºæ¨¡å‹æƒé‡
rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(model_path)
archive_data = future.result()

with open("math-model-final.tar.gz", "wb") as f:
    f.write(archive_data)
print("Model exported successfully!")

# 5. ä¿å­˜è¯„ä¼°ç»“æœ
results_summary = {
    'model_path': model_path,
    'test_accuracy': eval_results['accuracy'],
    'test_size': len(test_data),
    'model_name': 'math-model-final'
}

with open("model_evaluation.json", "w") as f:
    json.dump(results_summary, f, indent=2)
```

---

## ğŸ“ æ€»ç»“

### æ¨¡å‹å¯¼å‡º
- âœ… ä½¿ç”¨ `download_checkpoint_archive_from_tinker_path()` å¯¼å‡º
- âœ… å¯¼å‡ºçš„æ˜¯ LoRA adapter æƒé‡ï¼ˆ.tar.gz æ ¼å¼ï¼‰
- âœ… å¯ä»¥ä¸ HuggingFaceã€vLLM ç­‰æ¡†æ¶é›†æˆ
- âœ… å¯ä»¥åˆå¹¶æƒé‡ç”¨äºéƒ¨ç½²

### è¯„ä¼°æ–¹æ³•
- âœ… è®­ç»ƒä¸­è¯„ä¼°ï¼šç›‘æ§è®­ç»ƒæŸå¤±
- âœ… æµ‹è¯•é›†è¯„ä¼°ï¼šè®¡ç®—å‡†ç¡®ç‡ç­‰æŒ‡æ ‡
- âœ… Tinker Cookbookï¼šæä¾›é«˜çº§è¯„ä¼°å·¥å…·
- âœ… InspectAI é›†æˆï¼šæ ‡å‡†åŸºå‡†æµ‹è¯•
- âœ… è‡ªå®šä¹‰è¯„ä¼°ï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡

### æ¨èå·¥ä½œæµç¨‹
1. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80/20ï¼‰
2. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
3. å®šæœŸåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆæ¯ 5-10 epochsï¼‰
4. é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹
5. å¯¼å‡ºæ¨¡å‹æƒé‡
6. åœ¨å…¶ä»–å¹³å°éƒ¨ç½²å’Œä½¿ç”¨
