# Tinker å¾®è°ƒå®Œæ•´å·¥å…·åŒ…

å®Œæ•´çš„ Tinker API å¾®è°ƒå·¥å…·é›†ï¼ŒåŒ…å«è®­ç»ƒã€å¯¼å‡ºã€è¯„ä¼°çš„å…¨æµç¨‹æ”¯æŒã€‚

## ğŸ“ æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒè®­ç»ƒè„šæœ¬
1. **train_with_config.py** - æ¨èä½¿ç”¨çš„è®­ç»ƒè„šæœ¬
   - ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å‚æ•°
   - æ”¯æŒé•¿æ–‡æœ¬å¤„ç†ï¼ˆæœ€å¤§ 16384 tokensï¼‰
   - è‡ªåŠ¨ç»Ÿè®¡å’Œæˆªæ–­ç®¡ç†
   - è®­ç»ƒç»“æŸåå¯é€‰å¯¼å‡ºæ¨¡å‹

2. **tinker_finetune.py** - ç‹¬ç«‹è®­ç»ƒè„šæœ¬
   - æ‰€æœ‰é…ç½®åœ¨ä»£ç ä¸­
   - é€‚åˆå¿«é€Ÿæµ‹è¯•å’Œä¿®æ”¹

3. **config.py** - é…ç½®æ–‡ä»¶
   - æ•°æ®æ–‡ä»¶è·¯å¾„
   - æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
   - é•¿æ–‡æœ¬å¤„ç†è®¾ç½®
   - é‡‡æ ·å‚æ•°

### å·¥å…·è„šæœ¬
4. **split_dataset.py** - æ•°æ®é›†åˆ’åˆ†å·¥å…·
   - å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
   - æ”¯æŒå¤šä¸ªæ–‡ä»¶åˆå¹¶
   - å¯è®¾ç½®æµ‹è¯•é›†æ¯”ä¾‹
   - æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯

5. **evaluate_model.py** - æ¨¡å‹è¯„ä¼°è„šæœ¬
   - åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
   - è®¡ç®—å‡†ç¡®ç‡ç­‰æŒ‡æ ‡
   - æ˜¾ç¤ºé¢„æµ‹ç¤ºä¾‹
   - ä¿å­˜è¯„ä¼°ç»“æœ

### æ–‡æ¡£
6. **README.md** - ä¸»è¦ä½¿ç”¨æŒ‡å—
   - å¿«é€Ÿå¼€å§‹æ•™ç¨‹
   - å‚æ•°è¯¦ç»†è¯´æ˜
   - å¸¸è§é—®é¢˜è§£ç­”
   - ä½¿ç”¨ç¤ºä¾‹

7. **QUICK_START_LONG_TEXT.md** - é•¿æ–‡æœ¬å¿«é€ŸæŒ‡å—
   - é’ˆå¯¹ 16384 tokens é•¿è¾“å‡ºçš„é…ç½®
   - å†…å­˜ä¼˜åŒ–å»ºè®®
   - é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥
   - æ•…éšœæ’é™¤

8. **MODEL_EXPORT_EVALUATION.md** - å¯¼å‡ºå’Œè¯„ä¼°è¯¦ç»†æŒ‡å—
   - æ¨¡å‹å¯¼å‡ºæ–¹æ³•
   - ä¸å…¶ä»–æ¡†æ¶é›†æˆ
   - è¯„ä¼°ç­–ç•¥å’Œæ–¹æ³•
   - å®Œæ•´ç¤ºä¾‹ä»£ç 

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨ï¼ˆç®€å•ä»»åŠ¡ï¼‰

```bash
# 1. è®¾ç½® API å¯†é’¥
export TINKER_API_KEY=7624312694161539072

# 2. ç¼–è¾‘ config.pyï¼Œè®¾ç½®æ•°æ®è·¯å¾„
# DATA_FILES = ["data/s1k.json"]

# 3. è¿è¡Œè®­ç»ƒ
python train_with_config.py
```

### 2. å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰

```bash
# 1. è®¾ç½® API å¯†é’¥
export TINKER_API_KEY=7624312694161539072

# 2. åˆ’åˆ†æ•°æ®é›†
python split_dataset.py
# ç”Ÿæˆ: data/train_set.json, data/test_set.json

# 3. ç¼–è¾‘ config.py
# DATA_FILES = ["data/train_set.json"]

# 4. è®­ç»ƒæ¨¡å‹
python train_with_config.py
# è®°å½•è¾“å‡ºçš„ model_path

# 5. è¯„ä¼°æ¨¡å‹
# ç¼–è¾‘ evaluate_model.pyï¼Œè®¾ç½® MODEL_PATH
python evaluate_model.py
# æŸ¥çœ‹ evaluation_results.json
```

### 3. é•¿æ–‡æœ¬æ•°æ®ï¼ˆoutput > 4096 tokensï¼‰

```bash
# 1. æŸ¥çœ‹é•¿æ–‡æœ¬é…ç½®æŒ‡å—
cat QUICK_START_LONG_TEXT.md

# 2. ç¼–è¾‘ config.py
# MAX_SEQUENCE_LENGTH = 16384
# TRUNCATE_LONG_SEQUENCES = True

# 3. è¿è¡Œè®­ç»ƒï¼ˆæŸ¥çœ‹æˆªæ–­ç»Ÿè®¡ï¼‰
python train_with_config.py
```

## ğŸ“Š æ¨èå·¥ä½œæµç¨‹

### æ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ä½ çš„æ•°æ®ï¼‰

```python
# config.py æ¨èé…ç½®
MAX_SEQUENCE_LENGTH = 16384  # å®¹çº³é•¿è¯æ˜
TRUNCATE_LONG_SEQUENCES = True
DATA_FILES = ["data/train_set.json"]
BASE_MODEL = "Qwen/Qwen3-30B-A3B-Instruct"
LORA_RANK = 32
LEARNING_RATE = 5e-5
NUM_EPOCHS = 5
```

**æ­¥éª¤ï¼š**
1. ä½¿ç”¨ `split_dataset.py` åˆ’åˆ†æ•°æ®ï¼ˆ80% è®­ç»ƒï¼Œ20% æµ‹è¯•ï¼‰
2. ä½¿ç”¨ `train_with_config.py` è®­ç»ƒæ¨¡å‹
3. è§‚å¯ŸæŸå¤±ä¸‹é™å’Œåºåˆ—é•¿åº¦ç»Ÿè®¡
4. ä½¿ç”¨ `evaluate_model.py` åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
5. å¦‚æœéœ€è¦ï¼Œå¯¼å‡ºæ¨¡å‹æƒé‡ç”¨äºéƒ¨ç½²

## ğŸ¯ é’ˆå¯¹ä¸åŒåœºæ™¯çš„é…ç½®

### åœºæ™¯ 1: å°æ•°æ®é›†ï¼ˆ< 1000 æ ·æœ¬ï¼‰
```python
NUM_EPOCHS = 10  # æ›´å¤šè½®æ¬¡
LEARNING_RATE = 1e-4
LORA_RANK = 16  # è¾ƒå° rank é¿å…è¿‡æ‹Ÿåˆ
```

### åœºæ™¯ 2: å¤§æ•°æ®é›†ï¼ˆ> 10000 æ ·æœ¬ï¼‰
```python
NUM_EPOCHS = 3
LEARNING_RATE = 2e-4
LORA_RANK = 64  # æ›´å¤§å®¹é‡
```

### åœºæ™¯ 3: çŸ­æ–‡æœ¬ï¼ˆ< 512 tokensï¼‰
```python
MAX_SEQUENCE_LENGTH = 2048
NUM_EPOCHS = 5
```

### åœºæ™¯ 4: è¶…é•¿æ–‡æœ¬ï¼ˆ> 8192 tokensï¼‰
```python
MAX_SEQUENCE_LENGTH = 16384
LORA_RANK = 16  # å‡å°‘å†…å­˜ä½¿ç”¨
TRUNCATE_LONG_SEQUENCES = True
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒè¯•

### è®­ç»ƒæ—¶å…³æ³¨çš„æŒ‡æ ‡

1. **æŸå¤±å€¼ï¼ˆLossï¼‰**
   - åº”è¯¥é€æ¸ä¸‹é™
   - å¦‚æœéœ‡è¡ï¼šé™ä½å­¦ä¹ ç‡
   - å¦‚æœä¸ä¸‹é™ï¼šå¢åŠ å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ®

2. **åºåˆ—é•¿åº¦ç»Ÿè®¡**
   - Max: æœ€é•¿æ ·æœ¬é•¿åº¦
   - Mean/Median: å¤§éƒ¨åˆ†æ ·æœ¬é•¿åº¦
   - Truncated: è¢«æˆªæ–­çš„æ ·æœ¬æ•°

3. **å¤„ç†ç»Ÿè®¡**
   - Successfully processed: æˆåŠŸå¤„ç†çš„æ ·æœ¬
   - Truncated: è¢«æˆªæ–­çš„æ ·æœ¬
   - Skipped: è¢«è·³è¿‡çš„æ ·æœ¬
   - Failed: å¤„ç†å¤±è´¥çš„æ ·æœ¬

### å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜ï¼šå†…å­˜ä¸è¶³ (OOM)**
- é™ä½ MAX_SEQUENCE_LENGTH
- é™ä½ LORA_RANK
- ä½¿ç”¨æ›´å°çš„åŸºç¡€æ¨¡å‹

**é—®é¢˜ï¼šè®­ç»ƒå¤ªæ…¢**
- é™ä½ MAX_SEQUENCE_LENGTH
- å‡å°‘æ•°æ®é‡å…ˆå¿«é€Ÿæµ‹è¯•
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

**é—®é¢˜ï¼šæŸå¤±ä¸ä¸‹é™**
- æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
- å¢åŠ å­¦ä¹ ç‡
- å¢åŠ è®­ç»ƒè½®æ•°
- æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®

**é—®é¢˜ï¼šå¾ˆå¤šæ ·æœ¬è¢«æˆªæ–­**
- å¢åŠ  MAX_SEQUENCE_LENGTH
- æˆ–æ¥å—æˆªæ–­ï¼ˆé€šå¸¸å‰åŠéƒ¨åˆ†å·²åŒ…å«å…³é”®ä¿¡æ¯ï¼‰

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. å¤šé˜¶æ®µè®­ç»ƒ
```bash
# é˜¶æ®µ1: å¿«é€Ÿæµ‹è¯•ï¼ˆçŸ­åºåˆ—ï¼‰
# MAX_SEQUENCE_LENGTH = 4096, NUM_EPOCHS = 2
python train_with_config.py

# é˜¶æ®µ2: å®Œæ•´è®­ç»ƒï¼ˆé•¿åºåˆ—ï¼‰
# MAX_SEQUENCE_LENGTH = 16384, NUM_EPOCHS = 5
python train_with_config.py
```

### 2. ç»§ç»­è®­ç»ƒ
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨ load_state()
resume_path = "tinker://previous-model/state/final"
training_client.load_state(resume_path)
```

### 3. è‡ªå®šä¹‰è¯„ä¼°
ä¿®æ”¹ `evaluate_model.py` ä¸­çš„å‡½æ•°ï¼š
- `check_answer_exact_match()` - å®Œå…¨åŒ¹é…
- `check_answer_contains()` - åŒ…å«æ£€æŸ¥
- `extract_final_answer()` - ç­”æ¡ˆæå–

### 4. å¯¼å‡ºå’Œéƒ¨ç½²
```python
# å¯¼å‡ºæƒé‡
rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(model_path)
archive_data = future.result()

with open("model.tar.gz", "wb") as f:
    f.write(archive_data)

# ä½¿ç”¨ HuggingFace åŠ è½½
from peft import PeftModel
from transformers import AutoModelForCausalLM

base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-30B-A3B-Base")
model = PeftModel.from_pretrained(base_model, "./model_weights")
merged_model = model.merge_and_unload()
```

## ğŸ“– å»¶ä¼¸é˜…è¯»

- [Tinker å®˜æ–¹æ–‡æ¡£](https://tinker-docs.thinkingmachines.ai/)
- [Tinker Cookbook](https://github.com/thinking-machines-lab/tinker-cookbook)
- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)

## ğŸ’¡ æç¤ºå’ŒæŠ€å·§

1. **å…ˆå°è§„æ¨¡æµ‹è¯•**ï¼šç”¨å°‘é‡æ•°æ®å’ŒçŸ­åºåˆ—å¿«é€ŸéªŒè¯æµç¨‹
2. **ä¿å­˜ model_path**ï¼šè®­ç»ƒå®Œæˆåç«‹å³è®°å½• model_path
3. **å®šæœŸè¯„ä¼°**ï¼šæ¯ 5-10 epochs åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
4. **ç›‘æ§æˆªæ–­**ï¼šå¦‚æœè¶…è¿‡ 30% æ ·æœ¬è¢«æˆªæ–­ï¼Œè€ƒè™‘å¢åŠ  MAX_SEQUENCE_LENGTH
5. **ä½¿ç”¨ Instruct æ¨¡å‹**ï¼šå¯¹äºæŒ‡ä»¤éµå¾ªä»»åŠ¡ï¼ŒInstruct ç‰ˆæœ¬é€šå¸¸æ•ˆæœæ›´å¥½
6. **è°ƒæ•´æ¸©åº¦**ï¼šè®­ç»ƒç”¨ temperature=0.7ï¼Œè¯„ä¼°ç”¨ temperature=0.0

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœ‰é—®é¢˜ï¼š
1. æŸ¥çœ‹å¯¹åº”çš„ .md æ–‡æ¡£
2. æ£€æŸ¥ Tinker å®˜æ–¹æ–‡æ¡£
3. æŸ¥çœ‹è®­ç»ƒè¾“å‡ºçš„é”™è¯¯ä¿¡æ¯
4. è”ç³» Tinker æ”¯æŒ: tinker@thinkingmachines.ai

## âœ¨ æ›´æ–°æ—¥å¿—

- v1.0: åˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«è®­ç»ƒã€å¯¼å‡ºã€è¯„ä¼°å…¨æµç¨‹
- æ”¯æŒé•¿æ–‡æœ¬ï¼ˆæœ€å¤§ 16384 tokensï¼‰
- æä¾›å®Œæ•´çš„å·¥å…·å’Œæ–‡æ¡£
